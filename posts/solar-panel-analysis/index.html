<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Solar Panel Energy Yield: Part I | Marat's blog</title><meta name=keywords content><meta name=description content="Introduction
Have you ever wondered how many households in your neighborhood have a solar panel on their roof and
how much solar energy is being harnessed?
By combining the power of aerial imagery, solar radiation maps and machine learning we can address those
questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield.
I try to explain my idea step by step with many images and example calculations as possible."><meta name=author content><link rel=canonical href=https://kopytjuk.github.io/posts/solar-panel-analysis/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://kopytjuk.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kopytjuk.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kopytjuk.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://kopytjuk.github.io/apple-touch-icon.png><link rel=mask-icon href=https://kopytjuk.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://kopytjuk.github.io/posts/solar-panel-analysis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="https://kopytjuk.github.io/posts/solar-panel-analysis/"><meta property="og:site_name" content="Marat's blog"><meta property="og:title" content="Solar Panel Energy Yield: Part I"><meta property="og:description" content="Introduction Have you ever wondered how many households in your neighborhood have a solar panel on their roof and how much solar energy is being harnessed?
By combining the power of aerial imagery, solar radiation maps and machine learning we can address those questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield. I try to explain my idea step by step with many images and example calculations as possible."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-26T18:48:18+01:00"><meta property="article:modified_time" content="2025-02-26T18:48:18+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Solar Panel Energy Yield: Part I"><meta name=twitter:description content="Introduction
Have you ever wondered how many households in your neighborhood have a solar panel on their roof and
how much solar energy is being harnessed?
By combining the power of aerial imagery, solar radiation maps and machine learning we can address those
questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield.
I try to explain my idea step by step with many images and example calculations as possible."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kopytjuk.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Solar Panel Energy Yield: Part I","item":"https://kopytjuk.github.io/posts/solar-panel-analysis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Solar Panel Energy Yield: Part I","name":"Solar Panel Energy Yield: Part I","description":"Introduction Have you ever wondered how many households in your neighborhood have a solar panel on their roof and how much solar energy is being harnessed?\nBy combining the power of aerial imagery, solar radiation maps and machine learning we can address those questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield. I try to explain my idea step by step with many images and example calculations as possible.\n","keywords":[],"articleBody":"Introduction Have you ever wondered how many households in your neighborhood have a solar panel on their roof and how much solar energy is being harnessed?\nBy combining the power of aerial imagery, solar radiation maps and machine learning we can address those questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield. I try to explain my idea step by step with many images and example calculations as possible.\nIn the next blog post (Part II) we will run the method on a larger area (e.g. city or village) and discuss the results.\nAs a tiny appetizer, in the figure below you see an arial photograph of Titz, North Rhine-Westphalia with a almost-transparent overlay highlighting whether a solar panel is installed (green) or missing (purple):\nTable of contents Introduction Table of contents Datasets Aerial images Radiant exposure maps Data organization Methodology Extracting building outlines Cropping aerial images Detecting solar panels Estimating energy yield Building mask Solar panel mask Considering solar panel efficiency Computing solar panel area Summary and outlook Code Related projects References Appendix Detection vs. Segmentation Datasets The main source for the described approach is based on the publicly available datasets from the OpenGeodata.NRW portal. The OpenGeoData NRW portal is maintained by the state government of North Rhine-Westphalia (NRW) in Germany.\nThe portal offers a wide range of geospatial data, including high-resolution aerial images, LIDAR measurements, land-property and solar yield maps. Based on LIDAR data, also derived datasets such as digital surface and terrain models are provided.\nWe will focus on two particular data types, namely orthophotographs and radiant exposure maps, which we briefly explore in subsequent sections.\nAerial images Aerial photography (or airborne imagery) is the taking of photographs from an aircraft or other airborne platforms such as drones. An aerial image is usually converted to an orthophotograph in a process called orthorectification. Unlike an uncorrected aerial photograph, an orthophoto can be used to measure true distances. In other words, when we measure the distance between two trees, we can multiple the length value in pixels with a known scaling factor to obtain the true distance in meters.\nFor simplicity, I will use the term aerial image to refer to both aerial photographs and orthophotographs in this article. The image below shows an example of a residential area:\nFor further analysis (e.g. to find a building to a particular location) the images have to be geo-referenced. Georeferencing is the process of assigning real-world coordinates to digital maps or images, allowing them to be accurately positioned and overlaid within a geographic information system (GIS).\nLuckily, the coordinate information is already embedded in every image file allowing us to locate a particular spatial coordinate (e.g. a GPS longitude and latitude) to a single pixel. The image data and georeferencing metadata are stored in JPEG2000 format (.jp2). You can download and take a look at the images here.\nRadiant exposure maps In order to obtain the amount of solar energy a roof receives, we will use the radiant exposure bitmaps (in German: Solarkataster). Radiant exposure is the radiant energy received by a surface per unit area ($\\mathrm{J/m^2}$ in SI units) during a defined time period $\\Delta t$.\nThe image below shows an aerial image overlaid with radiant exposure map. Dark red areas indicate high energy yields whereas white color indicates poor yield areas.\nEach $0.5\\mathrm{m} \\times 0.5\\mathrm{m}$ pixel of this bitmap contains the estimated annual ($\\Delta t = 365*24\\textrm{h}$) radiant exposure in $\\frac{\\mathrm{kWh}}{\\mathrm{m^2}}$.\nA small numeric example: if a pixel with a exposure value of $1125 \\frac{\\mathrm{kWh}}{\\mathrm{m^2}}$ would overlap with a solar panel with 21% efficiency of the same size as the pixel ($ 0.5 \\cdot 0.5 \\mathrm{m^2} = 0.25 \\mathrm{m^2}$), that part of the solar panel would yield $1125 \\cdot 0.25 \\cdot 21\\% \\approx 59 \\mathrm{kWh}$ energy which is enough to boil (20°C) 491 litres of water annualy - should be enough for the daily dose of coffee. Alternatively we could charge the Tesla Model S 100kWh battery to 59%.\nThis amount of electricity costs ca. 23€ in Germany, 7€ in Hungary or 9€ in USA for a private household (as of March 2025).\nThe radiant exposure was estimated using solar radiation and weather (cloud) data by the German Weather Service combined with digital surface models. The latter alter the available solar radiation with respect to roof angles and obstacles (like trees or buildings) which reduce the annual solar yield further by casting shadows. More specific details on how those maps are created can be found in [1].\nThe bitmaps are available in 50cm and 1m resolution (true length of each pixel) as GeoTIFF files .tif. You can download the data here.\nData organization Before diving directly into technical details it is worth to spend a little time on understanding how the data (both aerial images and exposure maps) are organized and stored.\nThe area of state North Rhine-Westphalia is around $34000\\mathrm{km}^2$. Since storing all the data in a single file is not really realistic (think of a single aerial image with 10cm resolution per pixel), both the images and exposure maps are stored in tiles. A tile covers a square with 1km (for aerial images) or 4km (for exposure maps) side length.\nThe following image shows the 1km-tile coverage of aerial images of the city of Cologne:\nThe tile information is encoded in the file name, here a breakdown of a single aerial image dop10rgbi_32_280_5652_1_nw_2023.jp2:\ndop: digital orthophotograph (i.e. aerial image) 10: 10cm resolution rgbi: RGB with an infrared channel 32: UTM32 (EPSG 25832) projection 280: x-coordinate south west corner of the tile in km, here 280000m 5652: y-coordinate south west corner of the tile in km, here 5652000m 1: side length of the tile, here 1km nw: tile orientation, here north-west 2023: recording year Methodology The following workflow outlines the processing of a single 1km tile (here \"280_5652_1\") with all the data sources and intermediate artifacts involved.\nWhite arrows symbolize the usage of available data (e.g. aerial images) whereas the arrows with color indicate the flow of artifacts within the processing chain. I.e. those artifacts change when another tile (spatial area) is processed, whereas the set of source aerial images stays the same.\nEnough said, let’s dive into the processing steps. We start with the first blue box, where given a tile extent all buildings, described by their address, exact location and polygon outlines (e.g. rectangles for simple 4-wall buildings) are extracted from OpenStreetMap (OSM).\nEach building is identified with a unique ID (we will just reuse the original way_id from OSM). The outline is used for cropping images and for extracting the solar yield.\nA square-shaped area around a single building is cropped from the large aerial image tiles. The images are passed to an Machine Learning (ML) based solar-panel detection model which assigns to each pixel a solar-panel existence probability.\nThe building outlines (i.e. the building roofs) combined with solar energy yield bitmaps provide the potential (i.e. maximum possible) amount of energy. The segmentation results are combined with solar energy yield data as well. Together with some assumptions about the solar panel efficiency we use both to estimate the actual energy yield, as harnessed annually.\nThe information flows into a final tabular dataset (table in grey) which can be further employed to answer our introductory questions about installation rates of solar panels.\nIn the subsequent sections we will dive deeper into the technical nuances of each step. Feel free to skip to Part 2 of this post (WIP) to see what we can do with the outputs of this process.\nExtracting building outlines First, we need to extract all buildings which exist in the desired tile (i.e. spatial area). For that, we will use the OSMnx library, a Python package to access street networks and other geo-spatial features (such as buildings in our case) from OSM:\nimport osmnx as ox # Fetch buildings from OpenStreetMap buildings_gdf = ox.features_from_bbox(bbox, tags={\"building\": True}) The result is a GeoDataFrame with the OSM way identifier as its index. Keeping the original identifier of the building, facilitates visual inspection with available OSM tools. For example, we can find the building with the ID 316174397 at https://www.openstreetmap.org/way/316174397.\nThe geometry column of the buildings_gdf holds the building outlines in WGS84 geodetic coordinates, i.e. it is a polygon represented as a sequence of (longitude, latitude) pairs. Note that the units of WGS84 coordinates are degrees and not (euclidean) lengths such as meters or feet.\nIn order to use the building geometries with the geographic data from OpenGeodata.NRW, they first need to be transformed to the UTM32N (Northern Hemisphere) coordinate system. The UTM-system (UTM32N is a subset) is a cartesian system, which can be used to measure lengths in meters.\nFor the transformation we implement two helpful functions which we will use across the project:\nimport pyproj from shapely import ops as shapely_ops from shapely.geometry.base import BaseGeometry # WGS84 to UTM transformer_to_25832 = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:25832\", always_xy=True) def transform_wgs84_to_utm32N(geom: BaseGeometry) -\u003e BaseGeometry: return shapely_ops.transform(transformer_to_25832.transform, geom) The transform_wgs84_to_utm32N function can transform any of shapely’s geometry types (Point, LineString, Polygon) from WGS84 (i.e. longitude and latitude) to the desired UTM32 (x, y) coordinates. Using this function applied on the geometries in buildings_gdf we can compile a a tabular building overview dataset, which consists of a set of buildings with following attributes:\nbuilding ID OSM way ID address (addr:street, addr:housenumber, addr:postcode etc.) building outline in UTM32 coordinates (as a shapely.Polygon) The following image shows the first two entries of the building overview data:\nThis dataframe is used as a starting point for the subsequent steps.\nCropping aerial images The ML-model which I selected for the solar-panel detection cannot process a full 1km aerial image since it was trained on smaller (ca. 50-100m) images. Therefore, we need to crop the aerial image into multiple smaller images, having each building of interest in its center with a small (5-15m) margin around. The margin provides some context to the model - without the margin an asphalt-black image is hard to assign to a dark roof or a dark patch of the highway.\nTo open the aerial image we will use the rasterio Python library. The library interprets both the pixel data as georeferencing information, which is is stored as metadata in the JPEG2000 file. The pixel data can be accessed with a .read() call in a 4xWxH tensor, with red, blue, green and infrared channels. W and H are the width and height of the image in pixel units.\nThe following script shows the steps required to crop a single building from the:\nimport rasterio building_outline: Polygon # from the previous step output_location = Path(\"./cropped-images\") aerial_image_path = \"dop10rgbi_32_280_5652_1_nw_2023.jp2\" with rasterio.open(aerial_image_path) as image_data: # transform pixel coordinates to UTM32 coordinates (georeference metadata) affine_transform_px_to_geo = image_data.transform bounding_box = create_squared_box_around(building_outline, margin_around_building=5.0) # pixel area to cut from (holds the bounding box of the cut) crop_window = rasterio.windows.from_bounds( *bounding_box.bounds, transform=affine_transform_px_to_geo, ) # read the image image_matrix = image_data.read(window=crop_window) # ... # store the image plt.imsave(\"building-cut.png\", arr=image_matrix, dpi=200) Additionally to the cropped image itself, the affine transformation from UTM coordinates to the pixel coordinates is stored. Together with image sizes it allows to compute the cropped area. This information will be used in the energy extraction step.\nThe result of the cropping logic is a folder with images (with building-IDs as filenames) and a overview.csv table which holds the transformations and sizes:\nThis folder is used as input for the solar-panel detector, which we discuss next.\nDetecting solar panels The solar panel detector is responsible to detect solar panels in (cropped) aerial images. For this project I extended the gabrieltseng/solar-panel-segmentation repository. The repository contains code for training and evaluating a deep-learning-based segmentation model which identifies the locations of solar panels from satellite (or aerial) imagery. The model is implemented with PyTorch.\nThe verb “detect” can sound confusing if you are experienced in computer vision, to be precise: the model solves a semantic segmentation task. Semantic segmentation is a computer vision task aimed at classifying each pixel in an image into a specific category or object. The final goal is to produce a dense pixel-wise segmentation map of an image, where each pixel is assigned to a specific class. For the solar-panel detector a single label - existence of a solar panel - is assigned.\nSince we are only interested in the installed area and not the number of individual solar panels semantic segmentation (instead of instance segmentation) is sufficient. Wikipedia provides a concise overview about the different segmentation types.\nA ML-based segmentation model usually has two parts: an encoder and a decoder. For the former a ResNet34 base was used. For the latter parts of U-Net architecture were implemented. For further details, please refer to segmenter.py.\nI chose the above project because it had a well structured code, detailed installation instructions and good segmentation performance: according to the author, the model achieves a precision of 98.8%, and a recall of 97.7% using a threshold of 0.5 on the test dataset.\nLet’s take a look at a single segmentation result:\nThe figure shows the input and outputs of the segmentation process. The left image is the input to the segmentation model. The red building outline is added for inspection purposes.\nThe bitmap in the middle shows the raw output from the model, where each pixel holds the probability for the existence of a solar panel, between 0 (dark blue) and 1 (yellow).\nIn order to employ the result for energy yield estimation we need a binary value for each pixel, indicating whether a solar panel is installed or not. A simple method of Thresholding is applied on top of the segmented image. Values above the threshold indicate that a solar panel is installed, values below indicate the contrary. The image on the right shows the result of using the threshold of 0.5, where yellow pixels indicate, that a solar panel is available.\nLooking at the result, we can see that some of the pixels are classified incorrectly. One reason for those mistakes can be the mismatch between the training data and the input data we are ingesting into the model after it is trained (i.e. the inference of the model).\nThe training data used for training the model is satellite imagery from United States Geological Survey (USGS), which provides extensive collection of publicly available high-resolution aerial orthoimagery from across the United States. It can be the case, that the roof materials and shapes in Germany are different to the ones in the US, leading to errors. There can also be a difference in camera hardware, which represents colors in a slightly different way.\nHowever, the result of the segmentation step is a folder with segmentation bitmaps with each bitmap corresponding to a single input image.\nSide note: prior to using a segmentation model, I also tried to use an object detection model, with low success.\nEstimating energy yield In this step we determine both the potential and actual (based on segmentation bitmaps) energy yield for each building of interest.\nThe potential yield is the total yield which is available on the roof from the solar radiation whereas the actual yield is the energy yield which is harnessed by the installed solar panels.\nSimilar to the aerial images, we can open the radiant exposure bitmaps (i.e. .tif) with rasterio and extract parts of the bitmap as numpy arrays. Having a 2-dimensional energy yield bitmap (in kWh/m2) for each pixel $\\mathbf E[i,j]$ (which we can open with ) we can compute the total energy yield $E$ (in kWh) by summing up the pixel values and multiplying it with the real world area of a single pixel $A_{px}$, i.e.\n$$ E = \\sum_{i, j \\in M} \\mathbf E [i,j] * A_{px} $$\nIn Python, the formula can be implemented with a np.where operation:\nenergy_yield_bitmap: np.ndarray # E mask: np.ndarray # mask M of the same shape masked_yield_bitmap = np.where(mask, energy_yield_bitmap, # take the original value np.zeros_like(energy_matrix)) # 0 kWh/m2 otherwise energy = masked_yield_bitmap.sum()*pixel_area # in kWh The key difference in computing potential and actual yield is the mask $M$.\nA mask is a set of pixel indices to include for the sum calculation. We will refer the building (-roof) mask and solar panel mask as $M_b$ and $M_s$ respectively. In case the roof fully is covered with solar panels, both masks are equal, i.e. $M_b = M_s$.\nIn the next two subsections, we briefly explore how to obtain those two masks.\nBuilding mask In the building extraction step we exported the building outline geometries. Those can be employed to identify the pixels which are within the buildings geometry. The following figure outlines the principle:\nThe first image shows both the energy yield bitmap as well as the building outline in yellow. We create a binary mask based on building outline in the second image. For the mask creation the rasterio.features.rasterize function (docs) is used, which creates a 2D numpy matrix base on one or multiple shapely geometries:\nbuilding_mask = rasterize( [building_polygon_utm], # building polygon (i.e. outline) out_shape=energy_matrix.shape, # shape of the yield bitmap transform=energy_cropped_image_trafo, # UTM to bitmap tranformation fill=0, # used for pixels outside the polygon default_value=1, # used for pixels covered by the polygon dtype=np.uint8, all_touched=True ).astype(bool) The building_mask represents the mask $M_b$, which indicates the potential areas (pixels) where energy can be mined. Together with the energy yield bitmap $\\mathbf E$ we can select the pixels relevant for the energy calculation which is shown in the third image.\nSolar panel mask Here we will use the segmentation bitmaps together with building mask to select pixels which are inside the building outline AND classified as solar panels. The following figure visualizes the key steps to obtain the mask:\nFirst we load the segmentation bitmaps (first image), apply a threshold (second image) and check if the pixels above the threshold are within the building outline. The resulting mask $M_s$ (third image) is used together with energy yield bitmap to select relevant pixels from the energy yield bitmap (last image).\nsegmentation_bitmap = Image.open(...) segmentation_values = np.asarray(segmentation_bitmap) threshold = 0.5 solar_panel_detected_mask = segmentation_values \u003e threshold solar_panel_mask = solar_panel_detected_mask \u0026 building_mask The solar_panel_mask represents the mask $M_s$, which includes the pixels where solar energy is actually mined. Note that the AND operation \u0026 reduces the false positive panel detections, since we filter out falsely detected areas as those in the north of the building.\nConsidering solar panel efficiency Note, that the most commercial solar panels which are used on roofs today have a conversion efficiency of around 21%. The conversion efficiency is the percentage of solar energy that is converted to electricity.\nefficiency = 0.21 mined_energy = energy * efficiency # in kWh Computing solar panel area $$ A_{\\textrm{solar}} = \\sum_{i, j \\in M_s} A_{px} $$\nSummary and outlook With the methodology outline above we are able to estimate the harnessed energy by combining both solar panel detections and radiant exposure maps.\nFor the particular building in the example we have:\nArea roof $A_\\textrm{roof} = 195.75\\mathrm{m^2}$ and area solar panels $A_\\textrm{installed} = 23.25\\mathrm{m^2}$ Highest possible yield $E_\\textrm{max} = 180,683 \\textrm{kWh}$ Estimated yield with installed solar panels (21% efficiency) $E_\\textrm{installed} = 5,747 \\textrm{kWh}$ The amount of harnessed energy is enough to charge a Model S 100kWh battery over 50 times. I leave the calculation for boiling water to the reader.\nNow, if we do the same calculation for all buildings in a village or city, we can compute some statistics:\nproportion of buildings with a solar panel installed average area of solar panels annual/daily energy yield But I leave that for the next blog post.\nThank you for taking the time to read this article! Don’t hesitate to reach out reach out if you have questions or suggestions. I’m always eager to connect and continue the conversation.\nCode You can find my code here: kopytjuk/solar-panel-coverage-nrw\nRelated projects https://www.appsilon.com/post/using-ai-to-detect-solar-panels-part-1 (uses segmentation) Stanford DeepSolar Microsoft’s Global Renewables Watch project, a living atlas that maps and measures all utility-scale solar and onshore wind installations on Earth using artificial intelligence (AI) and satellite imagery, employs conv-net image segmentation to find solar panels. The underlying methodology used in Global Renewables Watch is described in this paper. References [1] LANUV Info 43 (PDF)\nAppendix Detection vs. Segmentation In order to avoid training a completely new model, which is a task on its own, I initially was looking into existing projects with pre-trained models.\nI found ArielDrabkin/Solar-Panel-Detector, which contains the model weights, a CLI interface and even a Gradio GUI application. The app is hosted on Huggingface. Since it is a model which is trained to solve an object detection task, it outputs 2D bounding boxes of detected objects in the image.\nThe following figure shows an areal image from a sample building with two detections with the corresponding confidence scores:\nThe model is good enough to detects whether solar panels are installed, but deriving areas is not really possible, because the bounding boxes are not rotated. Of course, object detectors with rotated bounding boxes exist, but integrating them into the model is the same effort as using and training segmentation model from scratch, so I went with the segmentation approach.\n","wordCount":"3519","inLanguage":"en","datePublished":"2025-02-26T18:48:18+01:00","dateModified":"2025-02-26T18:48:18+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kopytjuk.github.io/posts/solar-panel-analysis/"},"publisher":{"@type":"Organization","name":"Marat's blog","logo":{"@type":"ImageObject","url":"https://kopytjuk.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kopytjuk.github.io/ accesskey=h title="Marat's blog (Alt + H)">Marat's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Solar Panel Energy Yield: Part I</h1><div class=post-meta><span title='2025-02-26 18:48:18 +0100 +0100'>February 26, 2025</span></div></header><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Have you ever wondered how many households in your neighborhood have a solar panel on their roof and
how much solar energy is being harnessed?</p><p>By combining the power of aerial imagery, solar radiation maps and machine learning we can address those
questions. In this post I will explore an approach of detecting solar panels and estimating their annual energy yield.
I try to explain my idea step by step with many images and example calculations as possible.</p><p>In the next blog post (Part II) we will run the method on a larger area (e.g. city or village) and discuss the results.</p><p>As a tiny appetizer, in the figure below you see an arial photograph of Titz, North Rhine-Westphalia with
a almost-transparent overlay highlighting whether a solar panel is installed (green) or missing (purple):</p><p><img alt=header loading=lazy src=/posts/solar-panel-analysis/header-319-5653.jpg></p><h2 id=table-of-contents>Table of contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ul><li><a href=#introduction>Introduction</a></li><li><a href=#table-of-contents>Table of contents</a></li><li><a href=#datasets>Datasets</a><ul><li><a href=#aerial-images>Aerial images</a></li><li><a href=#radiant-exposure-maps>Radiant exposure maps</a></li><li><a href=#data-organization>Data organization</a></li></ul></li><li><a href=#methodology>Methodology</a><ul><li><a href=#extracting-building-outlines>Extracting building outlines</a></li><li><a href=#cropping-aerial-images>Cropping aerial images</a></li><li><a href=#detecting-solar-panels>Detecting solar panels</a></li><li><a href=#estimating-energy-yield>Estimating energy yield</a><ul><li><a href=#building-mask>Building mask</a></li><li><a href=#solar-panel-mask>Solar panel mask</a></li><li><a href=#considering-solar-panel-efficiency>Considering solar panel efficiency</a></li><li><a href=#computing-solar-panel-area>Computing solar panel area</a></li></ul></li></ul></li><li><a href=#summary-and-outlook>Summary and outlook</a></li><li><a href=#code>Code</a></li><li><a href=#related-projects>Related projects</a></li><li><a href=#references>References</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#detection-vs-segmentation>Detection vs. Segmentation</a></li></ul></li></ul><h2 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h2><p>The main source for the described approach is based on the publicly available
datasets from the <a href=https://www.opengeodata.nrw.de/produkte/>OpenGeodata.NRW</a> portal.
The OpenGeoData NRW portal is maintained by the state government of North Rhine-Westphalia (NRW) in Germany.</p><p>The portal offers a wide range of geospatial data, including high-resolution aerial images,
LIDAR measurements, land-property and solar yield maps.
Based on LIDAR data, also derived datasets such as digital surface and terrain models are provided.</p><p>We will focus on two particular data types, namely <strong>orthophotographs</strong> and <strong>radiant exposure</strong> maps,
which we briefly explore in subsequent sections.</p><h3 id=aerial-images>Aerial images<a hidden class=anchor aria-hidden=true href=#aerial-images>#</a></h3><p>Aerial photography (or airborne imagery) is the taking of photographs from an aircraft or other airborne platforms such as drones.
An aerial image is usually converted to an <a href=https://en.wikipedia.org/wiki/Orthophoto><strong>orthophotograph</strong></a> in a process called
<strong>orthorectification</strong>. Unlike an uncorrected aerial photograph, an orthophoto can be used to measure true distances. In other words,
when we measure the distance between two trees, we can multiple the length value in pixels
with a known scaling factor to obtain the true distance in meters.</p><p>For simplicity, I will use the term <em>aerial image</em> to refer to both aerial photographs and orthophotographs in this article.
The image below shows an example of a residential area:</p><p><img alt=orthophoto loading=lazy src=/posts/solar-panel-analysis/orthophoto-example.jpg></p><p>For further analysis (e.g. to find a building to a particular location) the images have to be geo-referenced.
<a href=https://en.wikipedia.org/wiki/Georeferencing>Georeferencing</a> is the process of
assigning real-world coordinates to digital maps or images,
allowing them to be accurately positioned and overlaid within a geographic information system (GIS).</p><p>Luckily, the coordinate information is already embedded in every image file allowing us to locate a particular spatial coordinate (e.g. a GPS longitude and latitude) to a single pixel.
The image data and georeferencing metadata are stored in <a href=https://en.wikipedia.org/wiki/JPEG_2000>JPEG2000</a> format (<code>.jp2</code>).
You can download and take a look at the images <a href=https://www.opengeodata.nrw.de/produkte/geobasis/lusat/akt/dop/dop_jp2_f10/>here</a>.</p><h3 id=radiant-exposure-maps>Radiant exposure maps<a hidden class=anchor aria-hidden=true href=#radiant-exposure-maps>#</a></h3><p>In order to obtain the amount of solar energy a roof receives, we will
use the <strong>radiant exposure</strong> bitmaps (in German: <em>Solarkataster</em>).
Radiant exposure is the radiant energy received by a surface per unit area ($\mathrm{J/m^2}$ in SI units) during a defined time period $\Delta t$.</p><p>The image below shows an aerial image overlaid with radiant exposure map. Dark red areas indicate high energy yields whereas white color
indicates poor yield areas.</p><p><img alt=solar-yield loading=lazy src=/posts/solar-panel-analysis/solar-yield-example.jpg></p><p>Each $0.5\mathrm{m} \times 0.5\mathrm{m}$ pixel of this bitmap contains the estimated annual ($\Delta t = 365*24\textrm{h}$) radiant exposure in $\frac{\mathrm{kWh}}{\mathrm{m^2}}$.</p><p>A small numeric example: if a pixel with a exposure value of $1125 \frac{\mathrm{kWh}}{\mathrm{m^2}}$ would overlap with a solar panel
with 21% efficiency of the same size as the pixel ($ 0.5 \cdot 0.5 \mathrm{m^2} = 0.25 \mathrm{m^2}$),
that part of the solar panel would yield $1125 \cdot 0.25 \cdot 21\% \approx 59 \mathrm{kWh}$ energy
which is enough to boil (20°C) 491 litres of water annualy - should be enough for the daily dose of coffee.
Alternatively we could charge the Tesla Model S 100kWh battery to 59%.</p><p>This amount of electricity costs ca. 23€ in Germany, 7€ in Hungary or 9€ in USA for a private household (as of March 2025).</p><p>The radiant exposure was estimated using solar radiation and weather (cloud) data by the German Weather Service combined with
<a href=https://en.wikipedia.org/wiki/Digital_elevation_model>digital surface models</a>. The latter alter the available solar radiation with respect to roof angles
and obstacles (like trees or buildings) which reduce the annual solar yield further by casting shadows.
More specific details on how those maps are created can be found in [1].</p><p>The bitmaps are available in 50cm and 1m resolution (true length of each pixel) as GeoTIFF files <code>.tif</code>.
You can download the data <a href=https://www.opengeodata.nrw.de/produkte/umwelt_klima/energie/solarkataster/strahlungsenergie_50cm/>here</a>.</p><h3 id=data-organization>Data organization<a hidden class=anchor aria-hidden=true href=#data-organization>#</a></h3><p>Before diving directly into technical details it is worth to spend a little time on understanding how the data
(both aerial images and exposure maps) are organized and stored.</p><p>The area of state North Rhine-Westphalia is around $34000\mathrm{km}^2$. Since storing all the data in a single file is not
really realistic (think of a single aerial image with 10cm resolution per pixel), both the images and exposure maps
are stored in <strong>tiles</strong>. A tile covers a square with 1km (for aerial images) or 4km (for exposure maps) side length.</p><p>The following image shows the 1km-tile coverage of aerial images of the city of Cologne:</p><p><img alt=tiles-cologne loading=lazy src=/posts/solar-panel-analysis/tiles-cologne.jpg></p><p>The tile information is encoded in the file name, here a breakdown of a single
aerial image <code>dop10rgbi_32_280_5652_1_nw_2023.jp2</code>:</p><ul><li><code>dop</code>: digital orthophotograph (i.e. aerial image)</li><li><code>10</code>: 10cm resolution</li><li><code>rgbi</code>: RGB with an infrared channel</li><li><code>32</code>: UTM32 (EPSG 25832) projection</li><li><code>280</code>: x-coordinate south west corner of the tile in km, here 280000m</li><li><code>5652</code>: y-coordinate south west corner of the tile in km, here 5652000m</li><li><code>1</code>: side length of the tile, here 1km</li><li><code>nw</code>: tile orientation, here north-west</li><li><code>2023</code>: recording year</li></ul><h2 id=methodology>Methodology<a hidden class=anchor aria-hidden=true href=#methodology>#</a></h2><p>The following workflow outlines the processing of a single 1km tile (here <code>"280_5652_1"</code>) with all the data sources and intermediate artifacts involved.</p><p>White arrows symbolize the usage of available data (e.g. aerial images) whereas the arrows with color indicate the flow of artifacts within the processing chain.
I.e. those artifacts change when another tile (spatial area) is processed, whereas the set of source aerial images stays the same.</p><p><img alt=methodology loading=lazy src=/posts/solar-panel-analysis/methodology.png></p><p>Enough said, let&rsquo;s dive into the processing steps. We start with the first blue box, where given a tile extent all buildings, described by their address,
exact location and polygon outlines (e.g. rectangles for simple 4-wall buildings) are extracted from OpenStreetMap (OSM).</p><p>Each building is identified with a unique ID (we will just reuse the original <code>way_id</code> from OSM). The outline is used for cropping images and for extracting the solar yield.</p><p>A square-shaped area around a single building is cropped from the large aerial image tiles.
The images are passed to an Machine Learning (ML) based solar-panel detection model which assigns to each pixel a solar-panel existence probability.</p><p>The building outlines (i.e. the building roofs) combined with solar energy yield bitmaps provide the <strong>potential</strong> (i.e. maximum possible) amount of energy.
The segmentation results are combined with solar energy yield data as well.
Together with some assumptions about the solar panel efficiency we use both to estimate the <strong>actual</strong> energy yield, as harnessed annually.</p><p>The information flows into a final tabular dataset (table in grey) which can be further employed to answer
our introductory questions about installation rates of solar panels.</p><p>In the subsequent sections we will dive deeper into the technical nuances of each step.
Feel free to skip to <strong>Part 2</strong> of this post (<strong>WIP</strong>) to see what we can do with the outputs of this process.</p><h3 id=extracting-building-outlines>Extracting building outlines<a hidden class=anchor aria-hidden=true href=#extracting-building-outlines>#</a></h3><p>First, we need to extract all buildings which exist in the desired tile (i.e. spatial area). For that, we will use
the <a href=https://osmnx.readthedocs.io/en/stable/>OSMnx</a> library, a Python package to access street networks and other geo-spatial features (such as buildings in our case)
from OSM:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> osmnx <span style=color:#66d9ef>as</span> ox
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Fetch buildings from OpenStreetMap</span>
</span></span><span style=display:flex><span>buildings_gdf <span style=color:#f92672>=</span> ox<span style=color:#f92672>.</span>features_from_bbox(bbox, tags<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;building&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span></code></pre></div><p>The result is a <a href=https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html>GeoDataFrame</a>
with the OSM <code>way</code> identifier as its index. Keeping the original identifier of the building, facilitates visual inspection with available OSM tools.
For example, we can find the building with the ID <code>316174397</code> at <a href=https://www.openstreetmap.org/way/316174397>https://www.openstreetmap.org/way/316174397</a>.</p><p>The <code>geometry</code> column of the <code>buildings_gdf</code> holds the building outlines in WGS84 geodetic coordinates,
i.e. it is a polygon represented as a sequence of (longitude, latitude) pairs. Note that the units of WGS84 coordinates
are degrees and not (euclidean) lengths such as meters or feet.</p><p>In order to use the building geometries with the geographic data from OpenGeodata.NRW, they first need to be transformed
to the UTM32N (Northern Hemisphere) coordinate system.
The <a href=https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system>UTM</a>-system (UTM32N is a subset)
is a cartesian system, which can be used to measure lengths in meters.</p><p>For the transformation we implement two helpful functions which we will use across the project:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pyproj
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> shapely <span style=color:#f92672>import</span> ops <span style=color:#66d9ef>as</span> shapely_ops
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> shapely.geometry.base <span style=color:#f92672>import</span> BaseGeometry
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># WGS84 to UTM</span>
</span></span><span style=display:flex><span>transformer_to_25832 <span style=color:#f92672>=</span> pyproj<span style=color:#f92672>.</span>Transformer<span style=color:#f92672>.</span>from_crs(<span style=color:#e6db74>&#34;EPSG:4326&#34;</span>, 
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;EPSG:25832&#34;</span>, always_xy<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform_wgs84_to_utm32N</span>(geom: BaseGeometry) <span style=color:#f92672>-&gt;</span> BaseGeometry:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> shapely_ops<span style=color:#f92672>.</span>transform(transformer_to_25832<span style=color:#f92672>.</span>transform, geom)
</span></span></code></pre></div><p>The <code>transform_wgs84_to_utm32N</code> function can transform any of <a href=https://shapely.readthedocs.io/en/2.0.6/index.html>shapely</a>&rsquo;s geometry types (Point, LineString, Polygon) from WGS84 (i.e. longitude and latitude) to the desired UTM32 (x, y) coordinates. Using this function applied on the geometries in <code>buildings_gdf</code> we can compile a a tabular <strong>building overview</strong> dataset, which consists of a set of buildings with following attributes:</p><ul><li>building ID</li><li>OSM way ID</li><li>address (<code>addr:street</code>, <code>addr:housenumber</code>, <code>addr:postcode</code> etc.)</li><li>building outline in UTM32 coordinates (as a <code>shapely.Polygon</code>)</li></ul><p>The following image shows the first two entries of the building overview data:</p><p><img alt=building-overview loading=lazy src=/posts/solar-panel-analysis/building-overview-df.jpg></p><p>This dataframe is used as a starting point for the subsequent steps.</p><h3 id=cropping-aerial-images>Cropping aerial images<a hidden class=anchor aria-hidden=true href=#cropping-aerial-images>#</a></h3><p>The ML-model which I selected for the solar-panel detection cannot process a full 1km aerial image since it was trained on smaller (ca. 50-100m) images.
Therefore, we need to crop the aerial image into multiple smaller images, having each building of interest
in its center with a small (5-15m) margin around. The margin provides some context to the model - without the margin an asphalt-black image is hard to assign to a dark roof or a dark patch of the highway.</p><p>To open the aerial image we will use the <a href=https://rasterio.readthedocs.io/en/stable/>rasterio</a> Python library.
The library interprets both the pixel data as georeferencing information, which is is stored as metadata in the JPEG2000 file.
The pixel data can be accessed with a <code>.read()</code> call in a <code>4xWxH</code> tensor, with red, blue, green and infrared channels.
<code>W</code> and <code>H</code> are the width and height of the image in pixel units.</p><p>The following script shows the steps required to crop a single building from the:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> rasterio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>building_outline: Polygon <span style=color:#75715e># from the previous step</span>
</span></span><span style=display:flex><span>output_location <span style=color:#f92672>=</span> Path(<span style=color:#e6db74>&#34;./cropped-images&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>aerial_image_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;dop10rgbi_32_280_5652_1_nw_2023.jp2&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> rasterio<span style=color:#f92672>.</span>open(aerial_image_path) <span style=color:#66d9ef>as</span> image_data:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># transform pixel coordinates to UTM32 coordinates (georeference metadata)</span>
</span></span><span style=display:flex><span>    affine_transform_px_to_geo <span style=color:#f92672>=</span> image_data<span style=color:#f92672>.</span>transform
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    bounding_box <span style=color:#f92672>=</span> create_squared_box_around(building_outline, margin_around_building<span style=color:#f92672>=</span><span style=color:#ae81ff>5.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># pixel area to cut from (holds the bounding box of the cut)</span>
</span></span><span style=display:flex><span>    crop_window <span style=color:#f92672>=</span> rasterio<span style=color:#f92672>.</span>windows<span style=color:#f92672>.</span>from_bounds(
</span></span><span style=display:flex><span>                    <span style=color:#f92672>*</span>bounding_box<span style=color:#f92672>.</span>bounds,
</span></span><span style=display:flex><span>                    transform<span style=color:#f92672>=</span>affine_transform_px_to_geo,
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># read the image</span>
</span></span><span style=display:flex><span>    image_matrix <span style=color:#f92672>=</span> image_data<span style=color:#f92672>.</span>read(window<span style=color:#f92672>=</span>crop_window)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># store the image</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>imsave(<span style=color:#e6db74>&#34;building-cut.png&#34;</span>, arr<span style=color:#f92672>=</span>image_matrix, dpi<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>)
</span></span></code></pre></div><p>Additionally to the cropped image itself, the <strong>affine transformation</strong> from UTM coordinates to the pixel coordinates is stored.
Together with image sizes it allows to compute the cropped area. This information will be used in the energy extraction step.</p><p>The result of the cropping logic is a folder with images (with building-IDs as filenames) and a
<code>overview.csv</code> table which holds the transformations and sizes:</p><p><img alt=image-cropper-output loading=lazy src=/posts/solar-panel-analysis/image-cropper-output.jpg></p><p>This folder is used as input for the solar-panel detector, which we discuss next.</p><h3 id=detecting-solar-panels>Detecting solar panels<a hidden class=anchor aria-hidden=true href=#detecting-solar-panels>#</a></h3><p>The solar panel detector is responsible to detect solar panels in (cropped) aerial images. For this project
I extended the <a href=https://github.com/gabrieltseng/solar-panel-segmentation>gabrieltseng/solar-panel-segmentation</a>
repository. The repository contains code for training and evaluating a deep-learning-based segmentation model
which identifies the locations of solar panels from satellite (or aerial) imagery.
The model is implemented with <a href=https://pytorch.org/>PyTorch</a>.</p><p>The verb &ldquo;detect&rdquo; can sound confusing if you are experienced in computer vision,
to be precise: the model solves a <strong>semantic segmentation</strong> task.
Semantic segmentation is a computer vision task aimed at classifying each pixel in an image into a specific category or object.
The final goal is to produce a dense pixel-wise segmentation map of an image, where each pixel is assigned to a specific class.
For the solar-panel detector a single label - existence of a solar panel - is assigned.</p><p>Since we are only interested in the installed <strong>area</strong> and not the number of individual solar panels
<em>semantic</em> segmentation (instead of <em>instance</em> segmentation) is sufficient. Wikipedia <a href=(https://en.wikipedia.org/wiki/Image_segmentation#Groups_of_image_segmentation)>provides</a> a concise overview about the different segmentation types.</p><p>A ML-based segmentation model usually has two parts: an encoder and a decoder.
For the former a <a href=https://en.wikipedia.org/wiki/Residual_neural_network>ResNet34</a> base was used.
For the latter parts of <a href=https://en.wikipedia.org/wiki/U-Net>U-Net</a> architecture were implemented.
For further details, please refer to <a href=https://github.com/gabrieltseng/solar-panel-segmentation/blob/master/solarnet/models/segmenter.py>segmenter.py</a>.</p><p>I chose the above project because it had a well structured code, detailed installation instructions and good segmentation performance:
according to the author, the model achieves a precision of 98.8%, and a recall of 97.7%
using a threshold of 0.5 on the test dataset.</p><p>Let&rsquo;s take a look at a single segmentation result:</p><p><img alt=segmentation-example loading=lazy src=/posts/solar-panel-analysis/segmentation-example.jpg></p><p>The figure shows the input and outputs of the segmentation process. The left image is the input to the segmentation model. The red building outline is added for inspection purposes.</p><p>The bitmap in the middle shows the raw output from the model, where each pixel holds the probability for the existence of a solar panel, between 0 (dark blue) and 1 (yellow).</p><p>In order to employ the result for energy yield estimation we need a <strong>binary value</strong> for each pixel, indicating whether a solar panel is installed or not.
A simple method of <a href=https://en.wikipedia.org/wiki/Thresholding_(image_processing)>Thresholding</a> is applied on top of the segmented image.
Values above the threshold indicate that a solar panel is installed, values below indicate the contrary.
The image on the right shows the result of using the threshold of 0.5, where yellow pixels indicate, that a solar panel is available.</p><p>Looking at the result, we can see that some of the pixels are classified incorrectly. One reason for those mistakes can be the mismatch between the training data and the input data
we are ingesting into the model after it is trained (i.e. the <strong>inference</strong> of the model).</p><p>The training data used for training the model is satellite imagery from United States Geological Survey (USGS), which provides extensive collection of <a href=https://earthexplorer.usgs.gov/>publicly available</a>
high-resolution aerial orthoimagery from across the United States. It can be the case, that the roof materials and shapes in Germany are different to the ones in the US, leading to errors.
There can also be a difference in camera hardware, which represents colors in a slightly different way.</p><p>However, the result of the segmentation step is a folder with segmentation bitmaps with each bitmap corresponding to a single input image.</p><p>Side note: prior to using a segmentation model, <a href=#detection-vs-segmentation>I also tried</a> to use an object detection model, with low success.</p><h3 id=estimating-energy-yield>Estimating energy yield<a hidden class=anchor aria-hidden=true href=#estimating-energy-yield>#</a></h3><p>In this step we determine both the <strong>potential</strong> and <strong>actual</strong> (based on segmentation bitmaps)
energy yield for each building of interest.</p><p>The potential yield is the total yield which is available on the roof from the solar radiation whereas
the actual yield is the energy yield which is harnessed by the installed solar panels.</p><p>Similar to the aerial images, we can open the radiant exposure bitmaps (i.e. <code>.tif</code>) with <a href=https://rasterio.readthedocs.io/en/stable/>rasterio</a> and extract parts of the bitmap as <code>numpy</code> arrays.
Having a 2-dimensional energy yield bitmap (in kWh/m2) for each pixel $\mathbf E[i,j]$ (which we can open with )
we can compute the total energy yield $E$ (in kWh) by summing up the pixel values and multiplying it with the real world area of a single pixel $A_{px}$, i.e.</p><p>$$
E = \sum_{i, j \in M} \mathbf E [i,j] * A_{px}
$$</p><p>In Python, the formula can be implemented with a <code>np.where</code> operation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>energy_yield_bitmap: np<span style=color:#f92672>.</span>ndarray  <span style=color:#75715e># E</span>
</span></span><span style=display:flex><span>mask: np<span style=color:#f92672>.</span>ndarray  <span style=color:#75715e># mask M of the same shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>masked_yield_bitmap <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(mask, 
</span></span><span style=display:flex><span>                        energy_yield_bitmap,  <span style=color:#75715e># take the original value</span>
</span></span><span style=display:flex><span>                        np<span style=color:#f92672>.</span>zeros_like(energy_matrix))  <span style=color:#75715e># 0 kWh/m2 otherwise</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>energy <span style=color:#f92672>=</span> masked_yield_bitmap<span style=color:#f92672>.</span>sum()<span style=color:#f92672>*</span>pixel_area  <span style=color:#75715e># in kWh</span>
</span></span></code></pre></div><p>The key difference in computing potential and actual yield is the mask $M$.</p><p>A mask is a set of pixel indices to include for the sum calculation.
We will refer the building (-roof) mask and solar panel mask as $M_b$ and $M_s$ respectively.
In case the roof fully is covered with solar panels, both masks are equal, i.e. $M_b = M_s$.</p><p>In the next two subsections, we briefly explore how to obtain those two masks.</p><h4 id=building-mask>Building mask<a hidden class=anchor aria-hidden=true href=#building-mask>#</a></h4><p>In the <a href=#extracting-building-outlines>building extraction step</a> we exported the building outline geometries.
Those can be employed to identify the pixels which are within the buildings geometry. The following figure outlines the principle:</p><p><img alt=potential-energy-yield-extraction loading=lazy src=/posts/solar-panel-analysis/potential-energy-yield-extraction.png></p><p>The first image shows both the energy yield bitmap as well as the building outline in yellow.
We create a binary mask based on building outline in the second image.
For the mask creation the <code>rasterio.features.rasterize</code> function (<a href=https://rasterio.readthedocs.io/en/stable/api/rasterio.features.html#rasterio.features.rasterize>docs</a>) is used,
which creates a 2D numpy matrix base on one or multiple shapely geometries:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>building_mask <span style=color:#f92672>=</span> rasterize(
</span></span><span style=display:flex><span>    [building_polygon_utm],  <span style=color:#75715e># building polygon (i.e. outline)</span>
</span></span><span style=display:flex><span>    out_shape<span style=color:#f92672>=</span>energy_matrix<span style=color:#f92672>.</span>shape,  <span style=color:#75715e># shape of the yield bitmap</span>
</span></span><span style=display:flex><span>    transform<span style=color:#f92672>=</span>energy_cropped_image_trafo,  <span style=color:#75715e># UTM to bitmap tranformation</span>
</span></span><span style=display:flex><span>    fill<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,  <span style=color:#75715e># used for pixels outside the polygon</span>
</span></span><span style=display:flex><span>    default_value<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, <span style=color:#75715e># used for pixels covered by the polygon</span>
</span></span><span style=display:flex><span>    dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>uint8,
</span></span><span style=display:flex><span>    all_touched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>astype(bool)
</span></span></code></pre></div><p>The <code>building_mask</code> represents the mask $M_b$, which indicates the potential areas (pixels) where energy can be mined. Together with the energy yield
bitmap $\mathbf E$ we can select the pixels relevant for the energy calculation which is shown in the third image.</p><h4 id=solar-panel-mask>Solar panel mask<a hidden class=anchor aria-hidden=true href=#solar-panel-mask>#</a></h4><p>Here we will use the segmentation bitmaps together with building mask to select pixels which are inside the building outline AND classified as solar panels.
The following figure visualizes the key steps to obtain the mask:</p><p><img alt=actual-energy-yield-extraction loading=lazy src=/posts/solar-panel-analysis/actual-energy-yield-extraction.png></p><p>First we load the segmentation bitmaps (first image), apply a threshold (second image) and check if the pixels above the threshold are within the building outline.
The resulting mask $M_s$ (third image) is used together with energy yield bitmap to select relevant pixels from the energy yield bitmap (last image).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>segmentation_bitmap <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(<span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>segmentation_values <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(segmentation_bitmap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>threshold <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>solar_panel_detected_mask <span style=color:#f92672>=</span> segmentation_values <span style=color:#f92672>&gt;</span> threshold
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>solar_panel_mask <span style=color:#f92672>=</span> solar_panel_detected_mask <span style=color:#f92672>&amp;</span> building_mask
</span></span></code></pre></div><p>The <code>solar_panel_mask</code> represents the mask $M_s$, which includes the pixels where solar energy is actually mined.
Note that the AND operation <code>&</code> reduces the false positive panel detections,
since we filter out falsely detected areas as those in the north of the building.</p><h4 id=considering-solar-panel-efficiency>Considering solar panel efficiency<a hidden class=anchor aria-hidden=true href=#considering-solar-panel-efficiency>#</a></h4><p>Note, that the most commercial solar panels which are used on roofs today have a conversion efficiency of <a href=https://css.umich.edu/publications/factsheets/energy/solar-pv-energy-factsheet>around 21%</a>.
The conversion efficiency is the percentage of solar energy that is converted to electricity.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>efficiency <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.21</span>
</span></span><span style=display:flex><span>mined_energy <span style=color:#f92672>=</span> energy <span style=color:#f92672>*</span> efficiency  <span style=color:#75715e># in kWh</span>
</span></span></code></pre></div><h4 id=computing-solar-panel-area>Computing solar panel area<a hidden class=anchor aria-hidden=true href=#computing-solar-panel-area>#</a></h4><p>$$
A_{\textrm{solar}} = \sum_{i, j \in M_s} A_{px}
$$</p><h2 id=summary-and-outlook>Summary and outlook<a hidden class=anchor aria-hidden=true href=#summary-and-outlook>#</a></h2><p>With the methodology outline above we are able to estimate the harnessed energy by combining both solar panel detections and radiant exposure maps.</p><p>For the particular building in the example we have:</p><ul><li>Area roof $A_\textrm{roof} = 195.75\mathrm{m^2}$ and area solar panels $A_\textrm{installed} = 23.25\mathrm{m^2}$</li><li>Highest possible yield $E_\textrm{max} = 180,683 \textrm{kWh}$</li><li>Estimated yield with installed solar panels (21% efficiency) $E_\textrm{installed} = 5,747 \textrm{kWh}$</li></ul><p>The amount of harnessed energy is enough to charge a Model S 100kWh battery over 50 times.
I leave the calculation for boiling water to the reader.</p><p>Now, if we do the same calculation for all buildings in a village or city, we can compute some statistics:</p><ul><li>proportion of buildings with a solar panel installed</li><li>average area of solar panels</li><li>annual/daily energy yield</li></ul><p>But I leave that for the next blog post.</p><p>Thank you for taking the time to read this article! Don&rsquo;t hesitate to reach out <a href=mailto:kopytjuk@mailbox.org>reach out</a>
if you have questions or suggestions.
I&rsquo;m always eager to connect and continue the conversation.</p><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><p>You can find my code here: <a href=https://github.com/kopytjuk/solar-panel-coverage-nrw>kopytjuk/solar-panel-coverage-nrw</a></p><h2 id=related-projects>Related projects<a hidden class=anchor aria-hidden=true href=#related-projects>#</a></h2><ul><li><a href=https://www.appsilon.com/post/using-ai-to-detect-solar-panels-part-1>https://www.appsilon.com/post/using-ai-to-detect-solar-panels-part-1</a> (uses segmentation)</li><li><a href=https://deepsolar.web.app/>Stanford DeepSolar</a></li><li>Microsoft&rsquo;s <a href=https://www.globalrenewableswatch.org/atlas>Global Renewables Watch</a> project, a living atlas that maps and measures <strong>all</strong> utility-scale solar and onshore wind installations on Earth using artificial intelligence (AI) and satellite imagery, employs conv-net image segmentation to find solar panels. The underlying methodology used in Global Renewables Watch is described in this <a href=https://www.nature.com/articles/s41597-022-01499-9>paper</a>.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <a href=https://www.energieatlas.nrw.de/site/service/download_publikationen>LANUV Info 43</a> (PDF)</p><h2 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h2><h3 id=detection-vs-segmentation>Detection vs. Segmentation<a hidden class=anchor aria-hidden=true href=#detection-vs-segmentation>#</a></h3><p>In order to avoid training a completely new model, which is a task on its own, I initially was looking into existing projects with pre-trained models.</p><p>I found <a href=https://github.com/ArielDrabkin/Solar-Panel-Detector>ArielDrabkin/Solar-Panel-Detector</a>, which contains the model weights, a CLI interface and even a <a href=https://www.gradio.app/>Gradio</a> GUI application. The app is <a href=https://huggingface.co/spaces/ArielDrabkin/Solar-Panel-Detector>hosted</a> on Huggingface. Since it is a model which is trained to solve an <em>object detection</em> task,
it outputs 2D bounding boxes of detected objects in the image.</p><p>The following figure shows an areal image from a sample building with two detections with the corresponding confidence scores:</p><p><img alt=solar-panel-detector-example loading=lazy src=/posts/solar-panel-analysis/solar-panel-detector-example.png></p><p>The model is good enough to detects whether solar panels are installed, but deriving areas is not really possible, because the bounding boxes
are not rotated. Of course, object detectors with rotated bounding boxes exist, but integrating them into the model is the same effort as using
and training segmentation model from scratch, so I went with the segmentation approach.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://kopytjuk.github.io/>Marat's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>