<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An extensible and modular architecture for evaluation of dynamic systems in real world environments | Marat's blog</title>
<meta name=keywords content><meta name=description content="Introduction
Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - MSE), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding ground-truth (GT) in a video file for human analysis."><meta name=author content><link rel=canonical href=https://kopytjuk.github.io/posts/kpi-architecture/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://kopytjuk.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kopytjuk.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kopytjuk.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://kopytjuk.github.io/apple-touch-icon.png><link rel=mask-icon href=https://kopytjuk.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://kopytjuk.github.io/posts/kpi-architecture/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="https://kopytjuk.github.io/posts/kpi-architecture/"><meta property="og:site_name" content="Marat's blog"><meta property="og:title" content="An extensible and modular architecture for evaluation of dynamic systems in real world environments"><meta property="og:description" content="Introduction Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm’s performance (like mean squared error - MSE), but over the project’s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm’s output and the corresponding ground-truth (GT) in a video file for human analysis."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-21T11:00:00+01:00"><meta property="article:modified_time" content="2022-03-21T11:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="An extensible and modular architecture for evaluation of dynamic systems in real world environments"><meta name=twitter:description content="Introduction
Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - MSE), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding ground-truth (GT) in a video file for human analysis."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kopytjuk.github.io/posts/"},{"@type":"ListItem","position":2,"name":"An extensible and modular architecture for evaluation of dynamic systems in real world environments","item":"https://kopytjuk.github.io/posts/kpi-architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An extensible and modular architecture for evaluation of dynamic systems in real world environments","name":"An extensible and modular architecture for evaluation of dynamic systems in real world environments","description":"Introduction Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm\u0026rsquo;s performance (like mean squared error - MSE), but over the project\u0026rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm\u0026rsquo;s output and the corresponding ground-truth (GT) in a video file for human analysis.\n","keywords":[],"articleBody":"Introduction Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm’s performance (like mean squared error - MSE), but over the project’s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm’s output and the corresponding ground-truth (GT) in a video file for human analysis.\nIn this post I will outline a modular software architecture for the evaluation codebase, which is imho simple and straightforward to extend and maintain. It proved one’s worth across multiple projects I was involved in. The focus of this architecture is the assessment of dynamic algorithms (or systems) applied in the real world (such as robots, trackers or sensors) which experience unexpected behavior from the environment. In most cases this behaviour has to be further analyzed with additional metrics and plots.\nBasics For the sake of better understanding, I’ll introduce an artificial setting which helps to internalize the proposed concepts. Let’s focus on a mobile robot picking and delivering small boxes in a large warehouse. This warehouse is also filled with shelves, employees and pallet trucks which have to be detected and avoided.\nImage: Audi AG\nFirst, I’ll formalize the systems the architecture is tailored for. Afterwards, a set of possible subtasks and assessment examples for the mobile robot are presented to motivate the need for an extensible, application-independent evaluation suite.\nDescrete systems In this blog post I’ll focus on the assessment of dynamic (i.e. exhibiting continual change) and discrete systems, which generate an action or detection at specific points in time (also called timesteps). Moreover it will be assumed, that equidistant samples are available, i.e. outputs are generated in a (temporally) fixed output rate. This assumption rarely holds in real-world scenarios, where the output rate depends on system’s load, other processes and the state of the environment. Thus, for the assessment, it is often helpful to resample or interpolate the signal to equidistant timesteps. Those timesteps can be seen as the clock of the evaluation routine.\nUsually, for each timestep, there is ground-truth data, representing the desired behaviour of the system at particular points in time. For a mobile robot in logistics it can be a pre-planned and safe path, or for the object-detection task, the collection of ground-truth objects, which have to be detected (e.g. for safety reasons).\nThe system’s output signal $\\tilde{y}(t)$, the resampled version $\\hat y(t)$ and the corresponding ground-truth $y(t)$ can be visualized in a timeline:\nThe clock for the evaluation is represented discrete timesteps $t_1, …, t_n$. Note that there is no “rule them all” resampling method - it is engineers’ task to select an interpolation method valid for the particular signal of interest.\nAssessment examples Example 1: Localization In the first example, the robot’s output can be its perceived position which has to be compared with it’s actual position:\n$$ y_1(t) = \\begin{bmatrix} x_1(t) \\ x_2(t) \\ \\varphi(t) \\end{bmatrix} = \\begin{bmatrix} \\mathbf x(t) \\ \\varphi(t) \\end{bmatrix} $$\nThis assessment can be accomplished with a mean Euclidean distance $e(t) = ||\\mathbf{\\hat x}(t) - \\mathbf{x}(t)||_2$ over evaluation duration $T$:\n$$ MED = \\frac{1}{T}\\sum_{t=1}^T e(t) $$\nIf you are interested in other localization metrics in ADAS context, please refer to [2], where more sophisticated metrics (considering bounding boxes) are introduced.\nExample 2: Object tracking The next valuable assessment can be whether the system is detecting all objects (e.g. humans, pallets) within its own perception area to avoid them while navigating. The robot internally holds a list of detected objects, i.e.:\n$$ y_2(t) = \\mathcal{O}_t = \\lbrace o_1, o_2, \\dots, o_n \\rbrace_t $$\nEach object in the list is described by its unique ID, coordinates and heading, i.e. $o_i = [\\text{id}, \\mathbf x, \\varphi]$.\nFor the object tracking subtask, metrics like multiple object tracking precision (MOTP) or the multiple object tracking accuracy (MOTA) from [1] can be employed. The idea will be shortly presented below:\nSmall circles represent the actual objects (ground-truth) the large ones are the objects detected by the system (also called object hypotheses). MOTP metric counts the number of misdetections, false detections, ID changes etc. over time. The MOTA metric computes the mean length of black arrows (i.e. the distances between detected and actual objects). For example a constant distance offset between the hypotheses and GT can be attributed to a sensor misalignment.\nIn addition to numeric metrics, for a deeper scene understanding you probably want to visualize the object bounding boxes (red: hypotheses, blue: ground-truth) and sensor data for a particular timestep, similar to the image below[3]:\nSummary So far, a set of potential subtasks (path following and object detection) together with corresponding metrics was introduced. On the first glance they share little between each other - each one needs a specific treatment and its own implementation. In the next section I’ll try define a generic language for the evaluation problem and propose an application-independent, extensible architecture.\nArchitecture First, we will summarize all possible plots, metrics, visualizations and derivates as artefacts. Further we will group them into two categories:\nTimestep artefacts can be generated from data available at a particular timestep $t$. For example, to compute the distance $e(t)$ between perceived position and it’s actual position of the robot we only need the data from $t$. Another example is the previous image - you only need sensor data and the objects (both GT and hypotheses) from timestep $t$. Timeseries artefacts however, aggregate data multiple timesteps. For example, in order to compute the mean distance (MED) you need the distances $e(t)$ from all $t$s. Animations which involve multiple frames also involve data from multiple timesteps. For the sake of simplicity, I’ll focus on artefacts which involve all timesteps, i.e. a data list of length $T$. Further, let’s generalize both the output data from the system/algorithm under test, sensor data and the ground-truth data as EvaluationData. Instances of this class have a sample(t) method to retrieve the data from a particular timestep $t$.\nThe evaluation loop iterates over all timesteps, retrieves data from available data sources and generates timeseries-artefacts. Additionally, for the timeseries artefacts, the data has to be collected over time - you can think of a recording tape. After all timesteps are processed, the timeseries artefacts are created.\nAs you can see, the methods sample(t), generate() and record() are the core of the architecture.\nThe following class diagram shows the abstract classes EvaluationData, TimestepArtefact and TimeseriesArtefact and together with their implementations. You can inherit from the abstract classes whenever you need a new artefacts or a data source.\nYou can extend your codebase with new metrics and plots and pass the list of desired artefacts to the evaluation loop without touching the loop’s codebase itself. The majority of code changes happen in the implementations of the artefacts and data sources.\nMoreover you can replace the algorithm under test by providing a different implementation as it was done with AlgorithmOutputImpl1 and AlgorithmOutputImpl2 - you can even have them both at the same time for comparison. The only thing to extend, is to create a new family of artefacts supporting multiple algorithm outputs in the signature of generate.\nAlso, bote that you have full access to the whole data after feeding the Tape instance via record() , which simply logs the data to memory or disk.\nSummary Assessment and analysis of real world (discrete) systems can get more and more complex over time. The proposed architecture with a main loop over evaluation timesteps, a collection of data sources and two separate artefacts groups allows easy replacement, straightforward extensibility and modularity by introducing clean interfaces to retrieve and consume evaluation data.\nYou want to use this architecture template if the following is true:\nyou have a discrete and dynamic system under test you have multiple algorithm implementations for the same task you have a large number of evaluation artefacts and the collection certainly will grow over time Of course, the proposed structure is an overkill for small number of metrics for application in “noise-free” environments, where an extension towards more insights won’t be needed.\nPossible extensions Nobody is perfect - this fact also applies to the architecture above. Here a list of further possible extensions:\nTimeseries artefacts for variable sized (","wordCount":"1572","inLanguage":"en","datePublished":"2022-03-21T11:00:00+01:00","dateModified":"2022-03-21T11:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kopytjuk.github.io/posts/kpi-architecture/"},"publisher":{"@type":"Organization","name":"Marat's blog","logo":{"@type":"ImageObject","url":"https://kopytjuk.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kopytjuk.github.io/ accesskey=h title="Marat's blog (Alt + H)">Marat's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">An extensible and modular architecture for evaluation of dynamic systems in real world environments</h1><div class=post-meta><span title='2022-03-21 11:00:00 +0100 +0100'>March 21, 2022</span></div></header><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Evaluating complex systems operating in noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning, you just want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - <em>MSE</em>), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding <em>ground-truth</em> (GT) in a video file for human analysis.</p><p>In this post I will outline a modular software architecture for the evaluation codebase, which is imho simple and straightforward to extend and maintain. It proved one&rsquo;s worth across multiple projects I was involved in. The focus of this architecture is the assessment of dynamic algorithms (or systems) applied in the real world (such as robots, trackers or sensors) which experience unexpected behavior from the environment. In most cases this behaviour has to be further analyzed with additional metrics and plots.</p><h2 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h2><p>For the sake of better understanding, I&rsquo;ll introduce an artificial setting which helps to internalize the proposed concepts. Let&rsquo;s focus on a mobile robot picking and delivering small boxes in a large warehouse. This warehouse is also filled with shelves, employees and pallet trucks which have to be detected and avoided.</p><p><img alt=audi-smart-factory loading=lazy src=/posts/kpi-architecture/audi-smart-factory.jpg>
Image: Audi AG</p><p>First, I&rsquo;ll formalize the systems the architecture is tailored for. Afterwards, a set of possible subtasks and assessment examples for the mobile robot are presented to motivate the need for an extensible, application-independent evaluation suite.</p><h3 id=descrete-systems>Descrete systems<a hidden class=anchor aria-hidden=true href=#descrete-systems>#</a></h3><p>In this blog post I&rsquo;ll focus on the assessment of <em>dynamic</em> (i.e. exhibiting continual change) and <em>discrete</em> systems, which generate an action or detection at specific points in time (also called <em>timesteps</em>). Moreover it will be assumed, that <em>equidistant</em> samples are available, i.e. outputs are generated in a (temporally) fixed output rate. This assumption rarely holds in real-world scenarios, where the output rate depends on system&rsquo;s load, other processes and the state of the environment. Thus, for the assessment, it is often helpful to <em>resample</em> or <em>interpolate</em> the signal to equidistant timesteps. Those timesteps can be seen as the <em>clock</em> of the evaluation routine.</p><p>Usually, for each timestep, there is <em>ground-truth</em> data, representing the desired behaviour of the system at particular points in time. For a mobile robot in logistics it can be a pre-planned and safe path, or for the object-detection task, the collection of ground-truth objects, which have to be detected (e.g. for safety reasons).</p><p>The system&rsquo;s output signal $\tilde{y}(t)$, the resampled version $\hat y(t)$ and the corresponding ground-truth $y(t)$ can be visualized in a timeline:</p><p><img alt=discrete-timeseries loading=lazy src=/posts/kpi-architecture/discrete-timeseries.png></p><p>The clock for the evaluation is represented discrete timesteps $t_1, &mldr;, t_n$. Note that there is no &ldquo;rule them all&rdquo; resampling method - it is engineers&rsquo; task to select an interpolation method valid for the particular signal of interest.</p><h3 id=assessment-examples>Assessment examples<a hidden class=anchor aria-hidden=true href=#assessment-examples>#</a></h3><h4 id=example-1-localization>Example 1: Localization<a hidden class=anchor aria-hidden=true href=#example-1-localization>#</a></h4><p>In the first example, the robot&rsquo;s output can be its perceived position which has to be compared with it&rsquo;s actual position:</p><p>$$
y_1(t) = \begin{bmatrix} x_1(t) \
x_2(t) \
\varphi(t)
\end{bmatrix}
= \begin{bmatrix} \mathbf x(t) \
\varphi(t)
\end{bmatrix}
$$</p><p>This assessment can be accomplished with a <em>mean Euclidean distance</em> $e(t) = ||\mathbf{\hat x}(t) - \mathbf{x}(t)||_2$ over evaluation duration $T$:</p><p>$$
MED = \frac{1}{T}\sum_{t=1}^T e(t)
$$</p><p>If you are interested in other localization metrics in ADAS context, please refer to <a href=https://doi.org/10.3390/s21175855>[2]</a>, where more sophisticated metrics (considering bounding boxes) are introduced.</p><h4 id=example-2-object-tracking>Example 2: Object tracking<a hidden class=anchor aria-hidden=true href=#example-2-object-tracking>#</a></h4><p>The next valuable assessment can be whether the system is detecting all objects (e.g. humans, pallets) within its own perception area to avoid them while navigating. The robot internally holds a list of detected objects, i.e.:</p><p>$$
y_2(t) = \mathcal{O}_t = \lbrace o_1, o_2, \dots, o_n \rbrace_t
$$</p><p>Each object in the list is described by its unique ID, coordinates and heading, i.e. $o_i = [\text{id}, \mathbf x, \varphi]$.</p><p>For the object tracking subtask, metrics like <em>multiple object tracking precision</em> (MOTP) or the <em>multiple object tracking accuracy</em> (MOTA) from [1] can be employed. The idea will be shortly presented below:</p><p><img alt=motp loading=lazy src=/posts/kpi-architecture/motp.png></p><p>Small circles represent the actual objects (ground-truth) the large ones are the objects detected by the system (also called <em>object hypotheses</em>). MOTP metric counts the number of misdetections, false detections, ID changes etc. over time. The MOTA metric computes the mean length of black arrows (i.e. the distances between detected and actual objects). For example a constant distance offset between the hypotheses and GT can be attributed to a sensor misalignment.</p><hr><p>In addition to numeric metrics, for a deeper scene understanding you probably want to visualize the object bounding boxes (red: hypotheses, blue: ground-truth) and sensor data for a particular timestep, similar to the image below[3]:</p><p><img alt=kitti-example loading=lazy src=/posts/kpi-architecture/kitti-example.png></p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>So far, a set of potential subtasks (path following and object detection) together with corresponding metrics was introduced. On the first glance they share little between each other - each one needs a specific treatment and its own implementation. In the next section I&rsquo;ll try define a generic language for the evaluation problem and propose an application-independent, extensible architecture.</p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>First, we will summarize all possible plots, metrics, visualizations and derivates as <em>artefacts</em>. Further we will group them into two categories:</p><ul><li><em>Timestep artefacts</em> can be generated from data available at a particular timestep $t$. For example, to compute the distance $e(t)$ between perceived position and it&rsquo;s actual position of the robot we only need the data from $t$. Another example is the previous image - you only need sensor data and the objects (both GT and hypotheses) from timestep $t$.</li></ul><ul><li><em>Timeseries artefacts</em> however, aggregate data multiple timesteps. For example, in order to compute the mean distance (MED) you need the distances $e(t)$ from all $t$s. Animations which involve multiple frames also involve data from multiple timesteps. For the sake of simplicity, I&rsquo;ll focus on artefacts which involve all timesteps, i.e. a data list of length $T$.</li></ul><p>Further, let&rsquo;s generalize both the output data from the system/algorithm under test, sensor data and the ground-truth data as <code>EvaluationData</code>. Instances of this class have a <code>sample(t)</code> method to retrieve the data from a particular timestep $t$.</p><p>The evaluation loop iterates over all timesteps, retrieves data from available data sources and generates timeseries-artefacts. Additionally, for the timeseries artefacts, the data has to be collected over time - you can think of a recording tape. After all timesteps are processed, the timeseries artefacts are created.</p><p><img alt=sequence-diagram loading=lazy src=/posts/kpi-architecture/sequence-diagram.png></p><p>As you can see, the methods <code>sample(t)</code>, <code>generate()</code> and <code>record()</code> are the core of the architecture.</p><hr><p>The following class diagram shows the abstract classes <code>EvaluationData</code>, <code>TimestepArtefact</code> and <code>TimeseriesArtefact</code> and together with their implementations. You can inherit from the abstract classes whenever you need a new artefacts or a data source.</p><p><img alt=class-diagram loading=lazy src=/posts/kpi-architecture/class-diagram.png></p><p>You can extend your codebase with new metrics and plots and pass the list of desired artefacts to the evaluation loop without touching the loop&rsquo;s codebase itself. The majority of code changes happen in the implementations of the artefacts and data sources.</p><p>Moreover you can replace the algorithm under test by providing a different implementation as it was done with <code>AlgorithmOutputImpl1</code> and <code>AlgorithmOutputImpl2</code> - you can even have them both at the same time for comparison. The only thing to extend, is to create a new family of artefacts supporting multiple algorithm outputs in the signature of <code>generate</code>.</p><p>Also, bote that you have full access to the whole data after feeding the <code>Tape</code> instance via <code>record()</code> , which simply logs the data to memory or disk.</p><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><p>Assessment and analysis of real world (discrete) systems can get more and more complex over time. The proposed architecture with a main loop over evaluation timesteps, a collection of data sources and two separate artefacts groups allows easy replacement, straightforward extensibility and modularity by introducing clean interfaces to retrieve and consume evaluation data.</p><p>You want to use this architecture template if the following is true:</p><ul><li>you have a discrete and dynamic system under test</li><li>you have multiple algorithm implementations for the same task</li><li>you have a large number of evaluation artefacts and the collection certainly will grow over time</li></ul><p>Of course, the proposed structure is an overkill for small number of metrics for application in &ldquo;noise-free&rdquo; environments, where an extension towards more insights won&rsquo;t be needed.</p><h2 id=possible-extensions>Possible extensions<a hidden class=anchor aria-hidden=true href=#possible-extensions>#</a></h2><p>Nobody is perfect - this fact also applies to the architecture above. Here a list of further possible extensions:</p><ul><li><em>Timeseries artefacts</em> for variable sized (&lt;T) lists.</li><li>Configuration management for data sources and artefacts collections. Serializing data source and artefacts instances allows to reproduce the same plots and metrics at a different point of time.</li><li>Manage passing arguments to <code>generate</code>. Havin multiple data sources allows a combination of different data constellations. Currently the <code>generate</code> signature solely accepts a list of objects equal to the number of data sources.</li><li>Parallelization - you can split the main loop over $t$ into small batches to decrease the runtime of a single run.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Bernardin, K., Stiefelhagen, R. <em>Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics.</em> EURASIP Journal on Image and Video Processing (2008)</p><p>[2] Rehrl, K.; Gröchenig, S. <em>Evaluating Localization Accuracy of Automated Driving Systems.</em> Sensors 2021, 21, 5855.</p><p>[3] Yang, Bin & Luo, Wenjie & Urtasun, Raquel. (2019). <em>PIXOR: Real-time 3D Object Detection from Point Clouds.</em></p><p>The UML plots were generated with <a href=https://plantuml.com/en/>PlantUML</a>, you can find the sources on <a href=https://github.com/kopytjuk/kopytjuk.github.io/tree/main/content/posts/kpi-architecture>GitHub</a>, in the root folder of this post.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://kopytjuk.github.io/>Marat's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>