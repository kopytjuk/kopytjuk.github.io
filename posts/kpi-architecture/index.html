<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An extensible and modular architecture for evaluation of dynamical systems in real world environments | Marat's blog</title><meta name=keywords content><meta name=description content="Introduction Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - MSE), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding ground-truth (GT) in a video file."><meta name=author content><link rel=canonical href=https://kopytjuk.github.io/posts/kpi-architecture/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kopytjuk.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kopytjuk.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kopytjuk.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://kopytjuk.github.io/apple-touch-icon.png><link rel=mask-icon href=https://kopytjuk.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="An extensible and modular architecture for evaluation of dynamical systems in real world environments"><meta property="og:description" content="Introduction Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - MSE), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding ground-truth (GT) in a video file."><meta property="og:type" content="article"><meta property="og:url" content="https://kopytjuk.github.io/posts/kpi-architecture/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-21T11:00:00+01:00"><meta property="article:modified_time" content="2022-03-21T11:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="An extensible and modular architecture for evaluation of dynamical systems in real world environments"><meta name=twitter:description content="Introduction Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - MSE), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding ground-truth (GT) in a video file."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kopytjuk.github.io/posts/"},{"@type":"ListItem","position":2,"name":"An extensible and modular architecture for evaluation of dynamical systems in real world environments","item":"https://kopytjuk.github.io/posts/kpi-architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An extensible and modular architecture for evaluation of dynamical systems in real world environments","name":"An extensible and modular architecture for evaluation of dynamical systems in real world environments","description":"Introduction Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm\u0026rsquo;s performance (like mean squared error - MSE), but over the project\u0026rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm\u0026rsquo;s output and the corresponding ground-truth (GT) in a video file.","keywords":[],"articleBody":"Introduction Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm’s performance (like mean squared error - MSE), but over the project’s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm’s output and the corresponding ground-truth (GT) in a video file.\nIn this post I will outline a modular software archecture for the evaluation codebase, which is imho simple and straightforward to extend and maintain. It proved one’s worth across multiple projects I was involved in. The focus of this architecture is the assessment of dynamical algorithms (or systems) applied in the real world (such as robots, trackers or sensors) which experience unexpected behavior from the environment. In most cases this behaviour has to be further analyzed with additional metrics and plots.\nBasics For the sake of better unterstanding, I’ll introduce an artificial setting which helps to internalize the proposed concepts. Let’s select the logistics domain and focus on a mobile robot picking and delivering small boxes in a production site. This warehouse is also filled with employees, pallet trucks and other moving objects which are to be detected and to avoided.\nImage: Audi AG\nFirst, I’ll introduce a formal language discrete systems the architecture deals with. Later, a set of possible subtasks and assessment examples for the robot are presented to motivate the need for an extensible, application-independent evaluation tooling.\nDescrete systems In this post the focus lies on the assessment of discrete systems, which output an action or detection at specific points in time (timesteps). Moreover it will be assumed, that equidistant outputs are available, i.e. outputs are generated in a fixed output rate. This assumption rarely holds in real-world scenarios, where the output rate depends on system’s load, other processes and the state of the environment. Thus, for the assessment, it is often helpful to resample or interpolate the signal to equidistant timesteps. Those timesteps can be seen as the clock of the evaluation routine.\nUsually, for each timestep, there is a soft of ground-truth data, representing the desired behaviour of the system at particular point in time. For a mobile robot in logistics it can be a safe path, or for the object-detection task, the collection of ground-truth objects, which have to be tracked.\nThe system’s output signal $\\tilde{y}(t)$, the resampled version $\\hat y(t)$ and the corresponding ground-truth $y(t)$ can be visualized in a timeline:\nThe clock for the evaluation is represented discrete timesteps $t_1, …, t_n$. Note that there is no “rule them all” resampling method - it is engineers’ task to select an interpolation method valid for the particular signal of interest.\nAssessment examples Example 1: Localization In the first example, the robot’s output can be its perceived position which has to be compared with it’s actual position:\n$$ y_1(t) = \\begin{bmatrix} x_1(t) \\ x_2(t) \\ \\varphi(t) \\end{bmatrix} = \\begin{bmatrix} \\mathbf x(t) \\ \\varphi(t) \\end{bmatrix} $$\nThis assessment can be accomplished with a mean euclidean distance $e(t) = ||\\mathbf{\\hat x}(t) - \\mathbf{x}(t)||_2$ over evaluation duration $T$:\n$$ MED = \\frac{1}{T}\\sum_{t=1}^T e(t) $$\nIf you are interested in other localization metrics in ADAS context, please refer to [2], where more sophisticated metrics (considering bounding boxes) are introduced.\nExample 2: Object tracking The next valuable assessment can be whether the system is detecting all objects (e.g. humans, pallets) within its own perception area to avoid them while navigating. The robot internally holds a list of detected objects, i.e.:\n$$ y_2(t) = \\mathcal{O}_t = \\lbrace o_1, o_2, \\dots, o_n \\rbrace_t $$\nEach object in the list is described by its unique ID, coordinates and heading, i.e. $o_i = [\\text{id}, \\mathbf x, \\varphi]$.\nFor the object tracking subtask, metrics like multiple object tracking precision (MOTP) or the multiple object tracking accuracy (MOTA) from [1] can be employed. The idea will be shortly presented below:\nSmall circles represent the actual objects (ground-truth) the large ones are the objects detected by the system (also called object hypotheses). MOTP metric counts the number of misdetections, false detections, ID changes etc. over time. The MOTA metric computes the mean length of black arrows (i.e. the distances between detected and actual objects). For example a constant distance offset between the hypotheses and GT can be attributed to a sensor misalignment.\n In addition to numeric metrics, for a deeper scene understanding you probably want to visualize the object bounding boxes (red: hypotheses, blue: ground-truth) and sensor data for a particular timestep, similar to the image below[3]:\nSummary So far, a set of potental subtasks (path following and object detection) together with corresponding metrics was introduced. On the first glance they share little between each other - each one needs a specific treatment and its own implementation. In the next section I’ll try define a generic language for the evaluation problem and propose an application-independent, extensible architecture.\nArchitecture First, we will summarize all possible plots, metrics, visualizations and derivates as artifacts. Further we will group them into two categories:\n Timestep artifacts can be generated from data available at a particular timestep $t$. For example, to compute the distance $e(t)$ between perceived position and it’s actual position of the robot we only need the data from $t$. Another example is the previous image - you only need sensor data and the objects (both GT and hypotheses) from timestep $t$.   Timeseries artifacts however, aggregate data multiple timesteps. For example, in order to compute the mean distance (MED) you need the distances $e(t)$ from all $t$s. Animations which involve multiple frames also involve data from multiple timesteps. For the sake of simplicity, I’ll focus on artifacts which involve all timesteps, i.e. a data list of length $T$.  Further, let’s generalize both the output data from the system/algorithm under test, sensor data and the ground-truth data as EvaluationData. Instances of this class have a sample(t) method to retrieve the data from a particular timestep $t$.\nThe evaluation loop iterates over all timesteps, retrieves data from available data sources and generates timeseries-artifacts. Additionally, for the timeseries artifacts, the data has to be collected over time - you can think of a recording tape. After all timesteps are processed, the timeseries artifacts are created.\nAs you can see, the methods sample(t), generate() and record() are the core of the architecture.\n The following class diagram shows the abstract classes EvaluationData, TimestepArtifact and TimeseriesArtifact and together with their implementations. You can inherit from the abstract classes whenever you need a new artifact or a data source.\nYou can extend your codebase with new metrics and plots and pass the list of desired artifacts to the evaluation loop without touching the loop’s codebase itself. The majority of code changes happen in the implementations of the artifacts and data sources.\nMoreover you can replace the algorithm under test by providing a different implementation as it was done with AlgorithmOutputImpl1 and AlgorithmOutputImpl2 - you can even have them both at the same time for comparison. The only thing to extend, is to create a new family of artifacts supporting multiple algorithm outputs in the signature of generate.\nAlso, bote that you have full access to the whole data after feeding the Tape instance via record() , which simply logs the data to memory or disk.\nSummary Assessment and analysis of real world (discrete) systems can get more and more complex over time. The proposed architecture with a main loop over evaluation timesteps, a collection of data sources and two separate artifact groups allows easy replacement, straightforward extensibility and modularity by introducing clean interfaces to retreive and consume evaluation data.\nYou want to use this architecture template if the following is true:\n you have a discrete and dynamic system under test you have multiple algorithm implementations for the same task you have a large number of evaluation artifacts and the collection certainly will grow over time  Of course, the proposed structure is an overkill for small number of metrics for application in “noise-free” envirnoments, where an extension towards more insights won’t be needed.\nPossible extensions Nobody is perfect - this fact also applies to the architecture above. Here a list of further possible extensions:\n Timeseries artifacts for variable sized (Configuration management for data sources and artifact collections. Serializing data source and artifact instances allows to reproduce the same plots and metrics at a different point of time. Manage passing arguments to generate. Havin multiple data sources allows a combination of different data constellations. Currently the generate signature solely accepts a list of objects equal to the number of data sources. Parallelization - you can split the main loop over $t$ into small batches to decrease the runtime of a single run.  References [1] Bernardin, K., Stiefelhagen, R. Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics. EURASIP Journal on Image and Video Processing (2008)\n[2] Rehrl, K.; Gröchenig, S. Evaluating Localization Accuracy of Automated Driving Systems. Sensors 2021, 21, 5855.\n[3] Yang, Bin \u0026 Luo, Wenjie \u0026 Urtasun, Raquel. (2019). PIXOR: Real-time 3D Object Detection from Point Clouds.\nThe UML plots were generated with PlantUML, you can find the sources on GitHub, in the root folder of this post.\n","wordCount":"1565","inLanguage":"en","datePublished":"2022-03-21T11:00:00+01:00","dateModified":"2022-03-21T11:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kopytjuk.github.io/posts/kpi-architecture/"},"publisher":{"@type":"Organization","name":"Marat's blog","logo":{"@type":"ImageObject","url":"https://kopytjuk.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kopytjuk.github.io/ accesskey=h title="Marat's blog (Alt + H)">Marat's blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>An extensible and modular architecture for evaluation of dynamical systems in real world environments</h1><div class=post-meta><span title="2022-03-21 11:00:00 +0100 +0100">March 21, 2022</span></div></header><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Evaluating complex systems operating is noisy real world environments is hard. Especially if you are dealing with dynamic behaviour over time, the requirements to the evaluation toolchain grow with every novel insight. In the beginning you want to compute just a single number describing algorithm&rsquo;s performance (like mean squared error - <em>MSE</em>), but over the project&rsquo;s lifetime your requirements to the evaluation framework grow: at some point you wish to generate static visualizations, create reports over a time range, dig deep at some specific situations or simply render both the algorithm&rsquo;s output and the corresponding <em>ground-truth</em> (GT) in a video file.</p><p>In this post I will outline a modular software archecture for the evaluation codebase, which is imho simple and straightforward to extend and maintain. It proved one&rsquo;s worth across multiple projects I was involved in. The focus of this architecture is the assessment of dynamical algorithms (or systems) applied in the real world (such as robots, trackers or sensors) which experience unexpected behavior from the environment. In most cases this behaviour has to be further analyzed with additional metrics and plots.</p><h2 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h2><p>For the sake of better unterstanding, I&rsquo;ll introduce an artificial setting which helps to internalize the proposed concepts. Let&rsquo;s select the logistics domain and focus on a mobile robot picking and delivering small boxes in a production site. This warehouse is also filled with employees, pallet trucks and other moving objects which are to be detected and to avoided.</p><p><img loading=lazy src=audi-smart-factory.jpg alt=audi-smart-factory>
Image: Audi AG</p><p>First, I&rsquo;ll introduce a formal language discrete systems the architecture deals with. Later, a set of possible subtasks and assessment examples for the robot are presented to motivate the need for an extensible, application-independent evaluation tooling.</p><h3 id=descrete-systems>Descrete systems<a hidden class=anchor aria-hidden=true href=#descrete-systems>#</a></h3><p>In this post the focus lies on the assessment of <em>discrete</em> systems, which output an action or detection at specific points in time (timesteps). Moreover it will be assumed, that <em>equidistant</em> outputs are available, i.e. outputs are generated in a fixed output rate. This assumption rarely holds in real-world scenarios, where the output rate depends on system&rsquo;s load, other processes and the state of the environment. Thus, for the assessment, it is often helpful to <em>resample</em> or <em>interpolate</em> the signal to equidistant timesteps. Those timesteps can be seen as the <em>clock</em> of the evaluation routine.</p><p>Usually, for each timestep, there is a soft of ground-truth data, representing the desired behaviour of the system at particular point in time. For a mobile robot in logistics it can be a safe path, or for the object-detection task, the collection of ground-truth objects, which have to be tracked.</p><p>The system&rsquo;s output signal $\tilde{y}(t)$, the resampled version $\hat y(t)$ and the corresponding ground-truth $y(t)$ can be visualized in a timeline:</p><p><img loading=lazy src=discrete-timeseries.png alt=discrete-timeseries></p><p>The clock for the evaluation is represented discrete timesteps $t_1, &mldr;, t_n$. Note that there is no &ldquo;rule them all&rdquo; resampling method - it is engineers&rsquo; task to select an interpolation method valid for the particular signal of interest.</p><h3 id=assessment-examples>Assessment examples<a hidden class=anchor aria-hidden=true href=#assessment-examples>#</a></h3><h4 id=example-1-localization>Example 1: Localization<a hidden class=anchor aria-hidden=true href=#example-1-localization>#</a></h4><p>In the first example, the robot&rsquo;s output can be its perceived position which has to be compared with it&rsquo;s actual position:</p><p>$$
y_1(t) = \begin{bmatrix} x_1(t) \
x_2(t) \
\varphi(t)
\end{bmatrix}
= \begin{bmatrix} \mathbf x(t) \
\varphi(t)
\end{bmatrix}
$$</p><p>This assessment can be accomplished with a <em>mean euclidean distance</em> $e(t) = ||\mathbf{\hat x}(t) - \mathbf{x}(t)||_2$ over evaluation duration $T$:</p><p>$$
MED = \frac{1}{T}\sum_{t=1}^T e(t)
$$</p><p>If you are interested in other localization metrics in ADAS context, please refer to <a href=https://doi.org/10.3390/s21175855>[2]</a>, where more sophisticated metrics (considering bounding boxes) are introduced.</p><h4 id=example-2-object-tracking>Example 2: Object tracking<a hidden class=anchor aria-hidden=true href=#example-2-object-tracking>#</a></h4><p>The next valuable assessment can be whether the system is detecting all objects (e.g. humans, pallets) within its own perception area to avoid them while navigating. The robot internally holds a list of detected objects, i.e.:</p><p>$$
y_2(t) = \mathcal{O}_t = \lbrace o_1, o_2, \dots, o_n \rbrace_t
$$</p><p>Each object in the list is described by its unique ID, coordinates and heading, i.e. $o_i = [\text{id}, \mathbf x, \varphi]$.</p><p>For the object tracking subtask, metrics like <em>multiple object tracking precision</em> (MOTP) or the <em>multiple object tracking accuracy</em> (MOTA) from [1] can be employed. The idea will be shortly presented below:</p><p><img loading=lazy src=motp.png alt=motp></p><p>Small circles represent the actual objects (ground-truth) the large ones are the objects detected by the system (also called <em>object hypotheses</em>). MOTP metric counts the number of misdetections, false detections, ID changes etc. over time. The MOTA metric computes the mean length of black arrows (i.e. the distances between detected and actual objects). For example a constant distance offset between the hypotheses and GT can be attributed to a sensor misalignment.</p><hr><p>In addition to numeric metrics, for a deeper scene understanding you probably want to visualize the object bounding boxes (red: hypotheses, blue: ground-truth) and sensor data for a particular timestep, similar to the image below[3]:</p><p><img loading=lazy src=kitti-example.png alt=kitti-example></p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>So far, a set of potental subtasks (path following and object detection) together with corresponding metrics was introduced. On the first glance they share little between each other - each one needs a specific treatment and its own implementation. In the next section I&rsquo;ll try define a generic language for the evaluation problem and propose an application-independent, extensible architecture.</p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>First, we will summarize all possible plots, metrics, visualizations and derivates as <em>artifacts</em>. Further we will group them into two categories:</p><ul><li><em>Timestep artifacts</em> can be generated from data available at a particular timestep $t$. For example, to compute the distance $e(t)$ between perceived position and it&rsquo;s actual position of the robot we only need the data from $t$. Another example is the previous image - you only need sensor data and the objects (both GT and hypotheses) from timestep $t$.</li></ul><ul><li><em>Timeseries artifacts</em> however, aggregate data multiple timesteps. For example, in order to compute the mean distance (MED) you need the distances $e(t)$ from all $t$s. Animations which involve multiple frames also involve data from multiple timesteps. For the sake of simplicity, I&rsquo;ll focus on artifacts which involve all timesteps, i.e. a data list of length $T$.</li></ul><p>Further, let&rsquo;s generalize both the output data from the system/algorithm under test, sensor data and the ground-truth data as <code>EvaluationData</code>. Instances of this class have a <code>sample(t)</code> method to retrieve the data from a particular timestep $t$.</p><p>The evaluation loop iterates over all timesteps, retrieves data from available data sources and generates timeseries-artifacts. Additionally, for the timeseries artifacts, the data has to be collected over time - you can think of a recording tape. After all timesteps are processed, the timeseries artifacts are created.</p><p><img loading=lazy src=sequence-diagram.png alt=sequence-diagram></p><p>As you can see, the methods <code>sample(t)</code>, <code>generate()</code> and <code>record()</code> are the core of the architecture.</p><hr><p>The following class diagram shows the abstract classes <code>EvaluationData</code>, <code>TimestepArtifact</code> and <code>TimeseriesArtifact</code> and together with their implementations. You can inherit from the abstract classes whenever you need a new artifact or a data source.</p><p><img loading=lazy src=class-diagram.png alt=class-diagram></p><p>You can extend your codebase with new metrics and plots and pass the list of desired artifacts to the evaluation loop without touching the loop&rsquo;s codebase itself. The majority of code changes happen in the implementations of the artifacts and data sources.</p><p>Moreover you can replace the algorithm under test by providing a different implementation as it was done with <code>AlgorithmOutputImpl1</code> and <code>AlgorithmOutputImpl2</code> - you can even have them both at the same time for comparison. The only thing to extend, is to create a new family of artifacts supporting multiple algorithm outputs in the signature of <code>generate</code>.</p><p>Also, bote that you have full access to the whole data after feeding the <code>Tape</code> instance via <code>record()</code> , which simply logs the data to memory or disk.</p><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><p>Assessment and analysis of real world (discrete) systems can get more and more complex over time. The proposed architecture with a main loop over evaluation timesteps, a collection of data sources and two separate artifact groups allows easy replacement, straightforward extensibility and modularity by introducing clean interfaces to retreive and consume evaluation data.</p><p>You want to use this architecture template if the following is true:</p><ul><li>you have a discrete and dynamic system under test</li><li>you have multiple algorithm implementations for the same task</li><li>you have a large number of evaluation artifacts and the collection certainly will grow over time</li></ul><p>Of course, the proposed structure is an overkill for small number of metrics for application in &ldquo;noise-free&rdquo; envirnoments, where an extension towards more insights won&rsquo;t be needed.</p><h2 id=possible-extensions>Possible extensions<a hidden class=anchor aria-hidden=true href=#possible-extensions>#</a></h2><p>Nobody is perfect - this fact also applies to the architecture above. Here a list of further possible extensions:</p><ul><li><em>Timeseries artifacts</em> for variable sized (&lt;T) lists.</li><li>Configuration management for data sources and artifact collections. Serializing data source and artifact instances allows to reproduce the same plots and metrics at a different point of time.</li><li>Manage passing arguments to <code>generate</code>. Havin multiple data sources allows a combination of different data constellations. Currently the <code>generate</code> signature solely accepts a list of objects equal to the number of data sources.</li><li>Parallelization - you can split the main loop over $t$ into small batches to decrease the runtime of a single run.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Bernardin, K., Stiefelhagen, R. <em>Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics.</em> EURASIP Journal on Image and Video Processing (2008)</p><p>[2] Rehrl, K.; Gröchenig, S. <em>Evaluating Localization Accuracy of Automated Driving Systems.</em> Sensors 2021, 21, 5855.</p><p>[3] Yang, Bin & Luo, Wenjie & Urtasun, Raquel. (2019). <em>PIXOR: Real-time 3D Object Detection from Point Clouds.</em></p><p>The UML plots were generated with <a href=https://plantuml.com/en/>PlantUML</a>, you can find the sources on <a href=https://github.com/kopytjuk/kopytjuk.github.io/tree/main/content/posts/kpi-architecture>GitHub</a>, in the root folder of this post.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kopytjuk.github.io/>Marat's blog</a></span>
<span>| <a href=disclaimer rel=noopener target=_blank>Disclaimer</a>
| Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>